miss_classfication_step3
##### step 4
probalities <- knearest(train,1, test) #repeat step 3 for K = 1 which is
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
1 - sum(diag(conf_mat))/sum(conf_mat)
miss_classfication_step4 <- 1 - sum(diag(conf_mat))/sum(conf_mat)
iss_classfication_step4
library(ggplot2)
library(kknn)
knearest=function(data,k,newdata)
{
n1=dim(data)[1]
n2=dim(newdata)[1]
p=dim(data)[2]
Prob=numeric(n2)
X = as.matrix(data[,-p])
Y = as.matrix(newdata[-p]) # change xn to Yn
X_hat = X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1)
Y_hat = Y/matrix(sqrt(rowSums(Y^2)), nrow = n2 , ncol = p - 1)
C <- X_hat %*% t(Y_hat)
D <- 1 - C #distacne matrix calculate
for (i in 1:n2 )
{
Ni <- order(D[,i])
N_i <- data[Ni[1:k],49]  # get k values
Prob[i] <- sum(N_i) / k
}
return(Prob) #return proabilities
}
##########################load excel file and divide data #######################
library(readxl)
data <- read_excel("spambase.xlsx")
data <- as.data.frame(data) # coonvert into data frame
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
test=data[-id,] #test data is newData
##### step 3
probalities <- knearest(train,5, test)
new_probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = new_probalities)
miss_classfication_step3 <-  1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
##### step 4
probalities <- knearest(train,1, test) #repeat step 3 for K = 1 which is
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
1 - sum(diag(conf_mat))/sum(conf_mat)
debugSource('~/Documents/projects/machine-learning-labs/machine-learning-lab1/assignment1.R', echo=TRUE)
View(D)
dim(D)
Ni[1:k]
library(ggplot2)
library(kknn)
knearest=function(data,k,newdata)
{
n1=dim(data)[1]
n2=dim(newdata)[1]
p=dim(data)[2]
Prob=numeric(n2)
X = as.matrix(data[,-p])
Y = as.matrix(newdata[-p]) # change xn to Yn
X_hat = X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1)
Y_hat = Y/matrix(sqrt(rowSums(Y^2)), nrow = n2 , ncol = p - 1)
C <- X_hat %*% t(Y_hat)
D <- 1 - C #distacne matrix calculate
for (i in 1:n2 )
{
Ni <- order(D[,i])
N_i <- data[Ni[1:k],"Spam"]  # get k values
Prob[i] <- sum(N_i) / k
}
return(Prob) #return proabilities
}
##########################load excel file and divide data #######################
library(readxl)
data <- read_excel("spambase.xlsx")
data <- as.data.frame(data) # coonvert into data frame
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
test=data[-id,] #test data is newData
####################################################################
##### step 3
probalities <- knearest(train,5, test)
new_probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = new_probalities)
miss_classfication_step3 <-  1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
##### step 4
probalities <- knearest(train,1, test) #repeat step 3 for K = 1 which is
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step4 <- 1 - sum(diag(conf_mat))/sum(conf_mat)
#### step 5
knn <- kknn(Spam ~. , train, test , k = 5 ) #standard kknn method at  K = 5
probalities <- knn$fitted.values
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step5 <- 1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
############## standard kknn for K = 1
knn <- kknn(Spam ~. , train, test , k = 1 )  #standard kknn method at  K = 5
probalities <- knn$fitted.values
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication <- 1 - sum(diag(conf_mat))/sum(conf_mat) / sum(conf_mat) #missclassfication
sprintf("knearest at k = 5 misclassfication value = %f ", miss_classfication_step3)
sprintf("standard kknn at k = 5 misclassfication value = %f ", miss_classfication_step5)
sprintf("knearest at k = 1 misclassfication value = %f ", miss_classfication_step4)
sprintf("standard kknn at k = 1 misclassfication value = %f ", miss_classfication)
#### step 6
ROC <- function(Y, Yfit, p){
m=length(p)
TPR=numeric(m)
FPR=numeric(m)
for(i in 1:m)
{
t <- table(Y,Yfit>p[i])
TPR[i] <-  t[2,2]/sum(t[2,])
FPR[i] <-  t[1,2]/sum(t[1,])
}
return (list(TPR=TPR,FPR=FPR))
}
pi_values <- seq(from = 0.05, to= 0.95 , by=0.05)
Y <- train[,ncol(data)]
kkn <-  kknn(Spam ~., train , test , k = 5) #built in Knn for k = 5
knearest_p <- knearest(train, 5 , test) #knearst k = 5
kkn_p <- kkn$fitted.values # knn proabilties
#debugonce(ROC)
roc_curve_knearest <- ROC(Y, knearest_p , pi_values)
roc_curve_kkn_p <- ROC(Y, kkn_p , pi_values)
#plot graoh
X<-  as.data.frame(roc_curve_knearest)
Y <- as.data.frame(roc_curve_kkn_p)
ggplot() +
geom_line(data = X, aes(x = X$FPR, y = X$TPR), color = "red") +
geom_line(data = Y, aes(x = Y$FPR, y = Y$TPR), color = "blue")+
ggtitle("ROC curve using kknn() & Knearnest()")+xlab("FPR") +ylab("TPR")
sensitivity_kn <- 1 - roc_curve_knearest$FPR
sensitivity_knn <- 1 - roc_curve_kkn_p$FPR
debugSource('~/Documents/projects/machine-learning-labs/machine-learning-lab1/assignment3.R', echo=TRUE)
lower_index_seq
X1
X2
K
k
Nfolds
sF
ind[i:j]
lower_index_seq
upper_index_seq
ind
dim(ind)
size(ind)
length(ind)
order(ind)
sort(ind)
Nfolds
X2
Y1
library(readxl)
library(ggplot2)
#qestion 4.1
data <- read_excel("tecator.xlsx") #load a data
data <- as.data.frame(data) # coonvert into data frame
ggplot(data, aes( x=data$Protein, y=data$Moisture) ) +
geom_point() + geom_smooth(method=lm)
#linear regression return response
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X=cbind(1,X)
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
Res = Xpred1%*%beta
return(Res)
}
#my cv function
myCV=function(X,Y,Nfolds){
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
# generating sequence
lower_index_seq <- seq(1,n,sF)
upper_index_seq <- seq(0,n,sF)
current_selected_feature <- which(model == 1)
X2<- X1[,current_selected_feature,drop=F] #apply k fold
for (k in 1:Nfolds)
{
i <- lower_index_seq[k]
j <- upper_index_seq[k+1]
k_fold_ind <- ind[i:j] # calculating indexes
Xpred <- X2[k_fold_ind,]
Xt <- X2[-k_fold_ind,]
Yp <- Y1[k_fold_ind]
Yt <- Y1[-k_fold_ind]
Ypred <- mylin(Xt,Yt, Xpred)
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
plot(x=Nfeat,y=MSE,  main = "MSE against their features", xlab = "Number of features", ylab ="MSE" ,
col = "red")
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
library(ggplot2)
library(kknn)
knearest=function(data,k,newdata)
{
n1=dim(data)[1]
n2=dim(newdata)[1]
p=dim(data)[2]
Prob=numeric(n2)
X = as.matrix(data[,-p])
Y = as.matrix(newdata[-p]) # change xn to Yn
X_hat = X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1)
Y_hat = Y/matrix(sqrt(rowSums(Y^2)), nrow = n2 , ncol = p - 1)
C <- X_hat %*% t(Y_hat)
D <- 1 - C #distacne matrix calculate
for (i in 1:n2 )
{
Ni <- order(D[,i])
N_i <- data[Ni[1:k],"Spam"]  # get k values
Prob[i] <- sum(N_i) / k
}
return(Prob) #return proabilities
}
##########################load excel file and divide data #######################
library(readxl)
data <- read_excel("spambase.xlsx")
data <- as.data.frame(data) # coonvert into data frame
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
test=data[-id,] #test data is newData
##### step 3
probalities <- knearest(train,5, test)
new_probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = new_probalities)
miss_classfication_step3 <-  1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
##### step 4
probalities <- knearest(train,1, test) #repeat step 3 for K = 1 which is
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step4 <- 1 - sum(diag(conf_mat))/sum(conf_mat)
#### step 5
knn <- kknn(Spam ~. , train, test , k = 5 ) #standard kknn method at  K = 5
probalities <- knn$fitted.values
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step5 <- 1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
library(ggplot2)
library(kknn)
knearest=function(data,k,newdata)
{
n1=dim(data)[1]
n2=dim(newdata)[1]
p=dim(data)[2]
Prob=numeric(n2)
X = as.matrix(data[,-p])
Y = as.matrix(newdata[-p]) # change xn to Yn
X_hat = X/matrix(sqrt(rowSums(X^2)), nrow=n1, ncol=p-1)
Y_hat = Y/matrix(sqrt(rowSums(Y^2)), nrow = n2 , ncol = p - 1)
C <- X_hat %*% t(Y_hat)
D <- 1 - C #distacne matrix calculate
for (i in 1:n2 )
{
Ni <- order(D[,i])
N_i <- data[Ni[1:k],"Spam"]  # get k values
Prob[i] <- sum(N_i) / k
}
return(Prob) #return proabilities
}
##########################load excel file and divide data #######################
library(readxl)
data <- read_excel("spambase.xlsx")
data <- as.data.frame(data) # coonvert into data frame
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
test=data[-id,] #test data is newData
##### step 3
probalities <- knearest(train,5, test)
new_probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = new_probalities)
miss_classfication_step3 <-  1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
##### step 4
probalities <- knearest(train,1, test) #repeat step 3 for K = 1 which is
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step4 <- 1 - sum(diag(conf_mat))/sum(conf_mat)
#### step 5
knn <- kknn(Spam ~. , train, test , k = 5 ) #standard kknn method at  K = 5
probalities <- knn$fitted.values
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication_step5 <- 1 - sum(diag(conf_mat))/sum(conf_mat) #missclassfication
############## standard kknn for K = 1
knn <- kknn(Spam ~. , train, test , k = 1 )  #standard kknn method at  K = 5
probalities <- knn$fitted.values
probalities <- ifelse(probalities > 0.5, 1,0)
conf_mat <- table(spam = test[,ncol(data)] , predicted_val = probalities)
miss_classfication <- 1 - sum(diag(conf_mat))/sum(conf_mat) / sum(conf_mat) #missclassfication
sprintf("knearest at k = 5 misclassfication value = %f ", miss_classfication_step3)
sprintf("standard kknn at k = 5 misclassfication value = %f ", miss_classfication_step5)
sprintf("knearest at k = 1 misclassfication value = %f ", miss_classfication_step4)
sprintf("standard kknn at k = 1 misclassfication value = %f ", miss_classfication)
ROC <- function(Y, Yfit, p){
m=length(p)
TPR=numeric(m)
FPR=numeric(m)
for(i in 1:m)
{
t <- table(Y,Yfit>p[i])
TPR[i] <-  t[2,2]/sum(t[2,])
FPR[i] <-  t[1,2]/sum(t[1,])
}
return (list(TPR=TPR,FPR=FPR))
}
debugSource('~/Documents/projects/machine-learning-labs/machine-learning-lab1/assignment1.R', echo=TRUE)
t
data.frame(label=c(1,0))
a<-data.frame(label=c(1,0))
row.names(a) <- c(1,0)
a
a<-data.frame(1=c(1,0))
a<-data.frame("1"=c(1,0))
row.names(a) <- c(1,0)
a
a<-data.frame(""=c(1,0))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c(1,0))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c(1,0), "FALSE"= c(1,0))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c(1,0), "FALSE"= c(1,0), "total"<-c("N+","N-"))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c(1,0), "FALSE"= c(1,0), "total"=c("N+","N-"))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c("TP","FN"), "FALSE"= c("FP","TN"), "total"=c("N+","N-"))
row.names(a) <- c(TRUE,FALSE)
a
a<-data.frame("TRUE"=c("TP","FP"), "FALSE"= c("FN","TN"), "total"=c("N+","N-"))
row.names(a) <- c(TRUE,FALSE)
a
library(kknn)
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
coef(fit)
summary(step)
#4.5
library(glmnet)
#4.5
library(glmnet)
#4.7
lambda.seq <- seq(0,1,0.001) #by default it will not consider zero
model=cv.glmnet(predictors, response, alpha=1,family="gaussian",lambda= lambda.seq)
model$lambda.min
coef(model)
library(readxl)
library(ggplot2)
#qestion 4.1
data <- read_excel("tecator.xlsx") #load a data
data <- as.data.frame(data) # coonvert into data frame
ggplot(data, aes( x=data$Protein, y=data$Moisture) ) +
geom_point() + geom_smooth(method=lm)
create_model_and_plot <- function(training, validation)
{
result <- list()
training_mse <- c()
validation_mse <- c()
for (i in 1:6)
{
#for traning data
model <- lm(Moisture ~ poly(Protein, degree = i, raw = TRUE) , data = training)
y_hat <- predict(model, training)
training_mse[i] <- mean((training$Moisture - y_hat)^2 )
# validation data fiting
y_hat <- predict(model,  validation)
validation_mse[i] <- mean((validation$Moisture - y_hat)^2)
}
ind <- seq(from=1,to=6,by=1)
result <- data.frame(training=training_mse, validation=validation_mse, x=ind)
}
# 4.3
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
validation=data[-id,] #test data is newData
result<- create_model_and_plot(train, validation)
ggplot()+ geom_line(aes(x=result$x,y=result$training, color ="training data" )) +
geom_line(aes(x =result$x, y = result$validation, color = "validation data"))+
ggtitle("MSE for poly Models")+xlab("upto 6 polynomial Degree") +ylab("Mean Square Error")
#4.4
library(MASS)
new_data <- data[2:102]
fit <- lm(Fat ~ . , data = new_data)
coef(fit)
step <- stepAIC(fit, direction = "both")
summary(step)
#4.5
library(glmnet)
response <- new_data$Fat
predictors <- as.matrix(new_data[1:100])
model10 <- glmnet(predictors ,response, alpha = 0, family = "gaussian"  ) #apha= 0 ridge regression
plot(model10, xvar="lambda", label=TRUE)
#4.6
#aplha 1 for lasso
model10 <- glmnet(predictors ,response, alpha = 1, family = "gaussian"  )
plot(model10, xvar="lambda", label=TRUE)
#4.7
lambda.seq <- seq(0,1,0.001) #by default it will not consider zero
model=cv.glmnet(predictors, response, alpha=1,family="gaussian",lambda= lambda.seq)
model$lambda.min
coef(model, s="lambda.min")
coef(model, s="lambda.min")
length(coef(model, s="lambda.min"))
library(readxl)
library(ggplot2)
#qestion 4.1
data <- read_excel("tecator.xlsx") #load a data
data <- as.data.frame(data) # coonvert into data frame
ggplot(data, aes( x=data$Protein, y=data$Moisture) ) +
geom_point() + geom_smooth(method=lm)
create_model_and_plot <- function(training, validation)
{
result <- list()
training_mse <- c()
validation_mse <- c()
for (i in 1:6)
{
#for traning data
model <- lm(Moisture ~ poly(Protein, degree = i, raw = TRUE) , data = training)
y_hat <- predict(model, training)
training_mse[i] <- mean((training$Moisture - y_hat)^2 )
# validation data fiting
y_hat <- predict(model,  validation)
validation_mse[i] <- mean((validation$Moisture - y_hat)^2)
}
ind <- seq(from=1,to=6,by=1)
result <- data.frame(training=training_mse, validation=validation_mse, x=ind)
}
# 4.3
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,] #train is data
validation=data[-id,] #test data is newData
result<- create_model_and_plot(train, validation)
ggplot()+ geom_line(aes(x=result$x,y=result$training, color ="training data" )) +
geom_line(aes(x =result$x, y = result$validation, color = "validation data"))+
ggtitle("MSE for poly Models")+xlab("upto 6 polynomial Degree") +ylab("Mean Square Error")
#4.4
library(MASS)
new_data <- data[2:102]
fit <- lm(Fat ~ . , data = new_data)
coef(fit)
step <- stepAIC(fit, direction = "both")
summary(step)
#4.5
library(glmnet)
response <- new_data$Fat
predictors <- as.matrix(new_data[1:100])
model10 <- glmnet(predictors ,response, alpha = 0, family = "gaussian"  ) #apha= 0 ridge regression
model10
summary(model10)
library(readxl)
library(ggplot2)
#qestion 4.1
data <- read_excel("tecator.xlsx") #load a data
data <- as.data.frame(data) # coonvert into data frame
ggplot(data, aes( x=data$Protein, y=data$Moisture) ) +
geom_point() + geom_smooth(method=lm)
poly(data$Protein, degree = 1, raw = TRUE)
poly(data$Protein, degree = 1, raw = FALSE)
lm(Moisture ~ poly(Protein, degree = i, raw = TRUE) , data = train)
lm(Moisture ~ poly(Protein, degree = i, raw = FALSE) , data = train)
debugSource('~/Documents/projects/machine-learning-labs/machine-learning-lab1/assignment3.R', echo=TRUE)
X1
X1[,current_selected_feature]
X1[,current_selected_feature],drop=f)
X1[,current_selected_feature],drop=F)
X1[,current_selected_feature],drop=F]
X1[,current_selected_feature,drop=F]
