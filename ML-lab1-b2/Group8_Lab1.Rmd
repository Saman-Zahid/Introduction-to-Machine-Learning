---
title: "Lab1 - Block2"
author: "Saman Zahid, Fahad Hameed, Rabnawaz Jansher"
date: "December 13, 2017"
output: pdf_document
---

# LAB 1 BLOCK 2: ENSEMBLE METHODS AND MIXTURE MODELS

# Assignment 1 ENSEMBLE METHODS

In this exercise we have use Adaboost classification trees and random forests to evauluate their performance on spam data. The data set have been divided into two parts, 2/3 for training and 1/3 as test data.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
spambase <- read.csv2("spambase.csv", header = TRUE, sep = ";", quote = "\"",
                      dec = ",", fill = TRUE)
spambase <- as.data.frame(spambase)

library(mboost)
library(randomForest)

```

### ADABOOST
```{r,  echo=FALSE}
n=dim(spambase)[1]
set.seed(12345)
id=sample(1:n, floor(n*2/3))
train=spambase[id,]
test=spambase[-id,]


number_of_trees <- seq(from = 10,to = 100, by = 10)

adaboost <- function(ntrees)
{
  fit <- blackboost(as.factor(Spam) ~., data = train,
           control = boost_control(mstop = ntrees, nu=0.1),
           family = AdaExp())
  
# misclassification test

ypredict <- predict(fit, newdata = test, type= "class")

conf_mat <- table(ypredict,test$Spam)

error_test <- 1-sum(diag(conf_mat))/sum(conf_mat)


}

error_rates_ada <- sapply(number_of_trees, adaboost)


plot(x=number_of_trees, y = error_rates_ada,type = "b",main="Adaboost Misclassification", xlab= "Number of Trees", ylab= "Error",
     col="blue", pch=19, cex=1)

```

The performance of the Adaboost classification trees can be seen above. We can see that the optimal would
be roughly 40 trees then it barely decreases as the number of trees grows. At 80 trees the test error seems to halt so if we want to create a substantially more complex model it may be preferable to use 80 trees. However, 40 trees whould be preffered choice as the test data stops decreasing after it.

Loss Function for the selected family is $Loss Function = e^{-y * f}$

### RANDOM FOREST

```{r, echo=FALSE}

random_forest <- function(ntrees)
{
  fit <- randomForest(as.factor(Spam) ~ ., data=train, importance=TRUE,
                      ntree = ntrees)
  
  # test misclassification
  ypredict <- predict(fit, test,type ="class")
  
  conf_mat <- table(ypredict,test$Spam)
  
  error_test <- 1-sum(diag(conf_mat))/sum(conf_mat)
}

error_rates_random <- sapply(number_of_trees, random_forest)

plot(x= number_of_trees , y =error_rates_random,type = "b",main="Random Forest Misclassification", xlab= "Number of Trees", ylab= "Error",
     col="blue", pch=19, cex=1)

```

The performance of the random forest can be seen above, the test error seem to stop decreasing after 40 trees and that should be the prefered choice. It can be seen that the test error increases as the number of
trees increases after 40 trees so the model have almost fit the training data perfectly with 40 trees.

### Performance Evaluation:

```{r}
#Misclassification for Adaboost
error_rates_ada

#Misclassification for Random Forest
error_rates_random
```

```{r echo=FALSE}


plot(y = error_rates_ada,x=number_of_trees, type = "l", col="red", 
     main= "Performance Evaluation of Adaboost Vs Random Forest", 
     xlab = "Number of Trees",ylab="Misclassification Rate", ylim = c(0,0.15))
points(y = error_rates_ada,x=number_of_trees,col="red", pch=19, cex=1)
lines(y = error_rates_random,x=number_of_trees, type= "l", col = "blue")
points(y = error_rates_random,x=number_of_trees,col="blue", pch=19, cex=1)
legend("topright",legend= c("adaboost","random forest"),
       col=c("red","blue"),lty=1,cex=0.8)

```

From plots, it is evident that the misclassification rate of random forest is much less than that of adaboost, therefore the performance of random forest is better

# Assignment 2 Mixture Model

```{r, echo=FALSE}
mixture_model <- function(my_k)
{
set.seed(1234567890)

max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data

for(n in 1:N) {

k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
  x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=my_k # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(j in 1:my_k) {
   mu[j,] <- runif(D,0.49,0.51)
}
pi
mu

for(it in 1:max_it)
  {
    
  Sys.sleep(0.5)
  # E-step: Computation of the fractional component assignment
  
  # Bernoulli distribution
  for (n in 1:N)
  {
    prob_x=0
  
    for (k in 1:K)
    {
      prob_x=prob_x+prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))) )*pi[k] #
    }
    
    for (k in 1:K)
    {
      
      z[n,k]=pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))) ) / prob_x
      
    }
    
  }
  
  #Log likelihood computation. 
  
  likelihood <-matrix(0,nrow =1000,ncol = K)

  llik[it] <-0
  for(n in 1:N)
  {
    
    for (k in 1:K)
    {
      likelihood[n,k] <- pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))))
    }
    llik[it]<- sum(log(rowSums(likelihood)))

  }
  
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console() 
  # Stop if the lok likelihood has not changed significantly
  if (it > 1) 
  {
    if (llik[it]-llik[it-1] < min_change)
    {
      if(K == 2)
      {
      plot(mu[1,], type="o", col="blue", ylim=c(0,1))
      points(mu[2,], type="o", col="red")
      }
      else if(K==3)
      {
        plot(mu[1,], type="o", col="blue", ylim=c(0,1))
        points(mu[2,], type="o", col="red")
        points(mu[3,], type="o", col="green")
      }
      else
      {
        plot(mu[1,], type="o", col="blue", ylim=c(0,1))
        points(mu[2,], type="o", col="red")
        points(mu[3,], type="o", col="green")
        points(mu[4,], type="o", col="yellow")
      }

    break
    }
  }
  
  #M-step: ML parameter estimation from the data and fractional component assignments 

  mu <- (t(z) %*% x) /colSums(z)
  
  # N - Total no. of observations
  pi <- colSums(z)/N
}


cat("value of updated pi is " , pi )
cat("\n")
sprintf("value of updated mu is") 
print(mu)

plot(llik[1:it], type="o")

}

```

In the Mixure Model task we used the following function:

## Mixture of multivariate Bernouilli distributions:

\begin{center}
${p(x) = \sum_{k=1}^N{\Pi}_kBern(x|k)}$
\end{center}

 \begin{center}
 $Bern(x|k)={\Pi}{\mu}_{k_i}^{x_i}(1-\mu_{k_{i}})^{(1-x_i)}$
 \end{center}
  
## Maximum Likelihood:

\begin{center} 
${\Sigma_{N}^{n=1}log(\Sigma_{N}^{n=1}\Pi_{k}N(X_{n}|\mu_{k},\Sigma_{k}))}$
\end{center}


## EM Algorithm:

### E-Step

\begin{center} 
${p(z_{nk}|X_{n},\mu,\pi) = \pi_{k} p (X_{n}|\mu_{k})/ \sum_{k}\mu_{k}p(X_{n}|\mu_{k})}$
\end{center}

### M-Step

\begin{center} 
${\pi^{ML}_{k} = \sum_{n}p(z_{nk}|X_{n},\mu,\pi)/N}$
\end{center}

\begin{center}
${\mu^{ML}_{ki} = \sum_{n}X_{ni}p(z_{nk}|X_{n},\mu,\pi)/\sum_{n}p(z_{nk}|X_{n},\mu,\pi)}$
\end{center}

### For K=2

* The First plot below shows the three Multivariate Bernoulli Distributions from which the data set have been generated.



* The Second plot shows two Multivariate Bernoulli Distributions estimated by the EM Algorithm. When k=2 Multivariate Bernoulli for each class has not affected EM much in order to find the other two distributions.



* Third plot shows log like-lihood versus the number of iterations.

```{r,echo=FALSE}
mixture_model(2)
```

### For K=3

Plot below shows three multivariate Bernoulli distributions estimated by the EM algorithm. The distributions are almost similar to the true ones with exceptation of the uniform one which have been influenced by the other two distributions.

```{r, echo=FALSE}
mixture_model(3)
```

### For K=4

The plot below shows four multivariate Bernoulli distributions estimated by the Expectation Maximization Algorithm. The blue and red curves are quite different from the true ones. EM algorithm have modelled two distributions and there are only three true distributions and fourth one is the most unpredictable.

```{r, echo=FALSE}

mixture_model(4)

```

For too few parameters that is for K=2, the logliklihood function runs for less iterations giving ${\mu}$ near to the true values of ${\mu}$ while for too many parameters the convergence steps increases. For K=3, the logliklihood value converges in neither too many nor too few steps, that is expected to provide result that is neither underfitted nor overfitted. For K=4 the convergence steps increases and the updated pi values for pi1 and pi2 differs greatly from the true value.


# APPENDIX
```{r,  eval=FALSE}

## Question 1

spambase <- read.csv2("spambase.csv", header = TRUE, sep = ";", quote = "\"",
                      dec = ",", fill = TRUE)
spambase <- as.data.frame(spambase)

### Adaboost

n=dim(spambase)[1]
set.seed(12345)
id=sample(1:n, floor(n*2/3))
train=spambase[id,]
test=spambase[-id,]


number_of_trees <- seq(from = 10,to = 100, by = 10)

adaboost <- function(ntrees)
{
  fit <- blackboost(as.factor(Spam) ~., data = train,
           control = boost_control(mstop = ntrees, nu=0.1),
           family = AdaExp())
  
# misclassification test

ypredict <- predict(fit, newdata = test, type= "class")

conf_mat <- table(ypredict,test$Spam)

error_test <- 1-sum(diag(conf_mat))/sum(conf_mat)


}

error_rates_ada <- sapply(number_of_trees, adaboost)
plot(error_rates_ada,type = "b",main="Adaboost Misclassification", xlab= "Number of Trees", ylab= "Error",
     col="blue", pch=19, cex=1)

# Loss Function = exp(-y * f)


## random forest
random_forest <- function(ntrees)
{
  fit <- randomForest(as.factor(Spam) ~ ., data=train, importance=TRUE,
                      ntree = ntrees)
  
  # test misclassification
  ypredict <- predict(fit, test,type ="class")
  
  conf_mat <- table(ypredict,test$Spam)
  
  error_test <- 1-sum(diag(conf_mat))/sum(conf_mat)
}

error_rates_random <- sapply(number_of_trees, random_forest)

plot(error_rates_random,type = "b",main="Random Forest Misclassification", xlab= "Number of Trees", ylab= "Error",
     col="blue", pch=19, cex=1)


#comparsion random forest Vs adBoost

plot(y = error_rates_ada,x=number_of_trees, type = "l", col="red", 
     main= "Performance Evaluation of Adaboost Vs Random Forest", 
     xlab = "Number of Trees",ylab="Misclassification Rate", ylim = c(0,0.15))
points(y = error_rates_ada,x=number_of_trees,col="red", pch=19, cex=1)
lines(y = error_rates_random,x=number_of_trees, type= "l", col = "blue")
points(y = error_rates_random,x=number_of_trees,col="blue", pch=19, cex=1)
legend("topright",legend= c("adaboost","random forest"),
       col=c("red","blue"),lty=1,cex=0.8)


## Question 2
mixture_model <- function(my_k)
{
set.seed(1234567890)

max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data

for(n in 1:N) {

k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
  x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
K=my_k # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(j in 1:my_k) {
   mu[j,] <- runif(D,0.49,0.51)
}
pi
mu

for(it in 1:max_it)
  {
  if(K == 2)
  {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  }
  else if(K==3)
  {
    plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    points(mu[2,], type="o", col="red")
    points(mu[3,], type="o", col="green")
  }
  else
  {
    plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    points(mu[2,], type="o", col="red")
    points(mu[3,], type="o", col="green")
    points(mu[4,], type="o", col="yellow")
  }

  Sys.sleep(0.5)
  # E-step: Computation of the fractional component assignment
  
  # Bernoulli distribution
  for (n in 1:N)
  {
    prob_x=0
  
    for (k in 1:K)
    {
      prob_x=prob_x+prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))) )*pi[k] #
    }
    
    for (k in 1:K)
    {
      
      z[n,k]=pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))) ) / prob_x
      
    }
    
  }
  
  #Log likelihood computation. 
  
  likelihood <-matrix(0,nrow =1000,ncol = K)

  llik[it] <-0
  for(n in 1:N)
  {
    
    for (k in 1:K)
    {
      likelihood[n,k] <- pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))))
    }
    llik[it]<- sum(log(rowSums(likelihood)))

  }
  
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console() 
  # Stop if the lok likelihood has not changed significantly
  if (it > 1) 
  {
    if (llik[it]-llik[it-1] < min_change)
    {
      if(K == 2)
      {
        plot(mu[1,], type="o", col="blue", ylim=c(0,1))
        points(mu[2,], type="o", col="red")
      }
      else if(K==3)
      {
        plot(mu[1,], type="o", col="blue", ylim=c(0,1))
        points(mu[2,], type="o", col="red")
        points(mu[3,], type="o", col="green")
      }
      else
      {
        plot(mu[1,], type="o", col="blue", ylim=c(0,1))
        points(mu[2,], type="o", col="red")
        points(mu[3,], type="o", col="green")
        points(mu[4,], type="o", col="yellow")
      }
      
      break
    }
  }
  
  #M-step: ML parameter estimation from the data and fractional component assignments 

  mu<- (t(z) %*% x) /colSums(z)
  
  # N - Total no. of observations
  pi <- colSums(z)/N
}

cat("value of updated pi is " , pi )
cat("\n")
sprintf("value of updated mu is") 
print(mu)

plot(llik[1:it], type="o")

}

mixture_model(2)
mixture_model(3)
mixture_model(4)

```