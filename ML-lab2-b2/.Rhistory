data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
library(kernlab)
x <- as.matrix(train[,-4703])
y <- train[,4703]
svm_fit <- ksvm(data = train,Conference ~ . ,kernel="vanilladot",
scaled = FALSE)
svm_fit
install.packages("kernlab")
install.packages("kernlab")
install.packages("kernlab")
str(as.list(.GlobalEnv))
install.packages("kernlab")
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
library(kernlab)
x <- as.matrix(train[,-4703])
y <- train[,4703]
svm_fit <- ksvm(data = train,Conference ~ . ,kernel="vanilladot",
scaled = FALSE)
svm_fit
ypred <- predict(svm_fit, newdata = test, type="response")
confusion_mat <- table(ypred,test$Conference)
misclas_svm <- 1 - sum (diag(confusion_mat))/sum(confusion_mat)
misclas_svm
save.image("image.RData")
misclas_svm
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
library(kernlab)
x <- as.matrix(train[,-4703])
y <- train[,4703]
svm_fit <- ksvm(data = train,Conference ~ . ,kernel="vanilladot",
scaled = FALSE)
ypred <- predict(svm_fit, newdata = test, type="response")
confusion_mat <- table(ypred,test$Conference)
misclas_svm <- 1 - sum (diag(confusion_mat))/sum(confusion_mat)
svm_fit
trace(setTimeLimit, quote(cat(paste("-", sys.calls(), collapse = "\n"))))
svm_fit
svm_fit <- ksvm(data = train,Conference ~ . ,kernel="vanilladot",
scaled = FALSE)
svm_fit
install.packages("kernlab", type="source")
install.packages("kernlab", type="source")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)
traceback()
sessionInfo()
.libPaths()
options(tz="Europe/Berlin")
options(tz="Europe/Berlin")
options(tz="Europe/Berlin")
options(tz="Europe/Berlin")
options(tz="Europe/Berlin")
Sys.time()
Sys.time()
Sys.time()
Sys.time()
Sys.setenv
Sys.setenv(TZ = "America/Los_Angeles")
Sys.setenv(TZ = "America/Los_Angeles")
Sys.time()
Sys.setenv(TZ = "America/Los_Angeles")
Sys.setenv(TZ = "America/Los_Angeles")
Sys.getenv("R_home")
R.home()
Sys.setenv(TZ = "America/Los_Angeles")
R.home(component = "home")
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
library(pamr)
rownames(train) <- 1:nrow(train)
which(colnames(train)=="Conference")
x <- t(train[,-4703])
y <- train[[4703]]
mydata <- list(x=x, y= as.factor(y), geneid=as.character(1:nrow(x)),genenames = rownames(x))
model_train <- pamr.train(mydata)
cv_model <- pamr.cv(model_train, data = mydata)
pamr.plotcv(cv_model) # legend is not shown in plot and only 2 lines are drawn why?
print(cv_model)
model_fit <- pamr.train(mydata, threshold = cv_model$threshold[which.min(cv_model$error)])
# par(mfrow=c(1,1),mar=c(2,2,2,2))
pamr.plotcen(model_train, mydata, threshold = cv_model$threshold[which.min(cv_model$error)])
features = pamr.listgenes(model_train, mydata, threshold = 1.306, genenames=TRUE)
cat( paste( colnames(train)[as.numeric(features[1:10,1])], collapse='\n' ) )
misclas <- pamr.confusion(cv_model, threshold = 1.306)
library(glmnet)
set.seed(12345)
response <- train$Conference
predictors <- as.matrix(train[,-4703])
elastic_model <- glmnet(x=predictors,y=response,family = "binomial",alpha = 0.5)
cv.fit <- cv.glmnet(x=predictors,y=response,family="binomial",alpha = 0.5)
cv.fit$lambda.min
# par(mar=c(3,3,3,3))
plot(cv.fit)
par(mar=c(3,3,3,3))
plot(cv.fit)
predictor_test <- as.matrix(test[,-4703])
ypredict <- predict(object = elastic_model,newx = predictor_test, s = cv.fit$lambda.min,
type = "class", exact = TRUE)
confusion_mat <- table(ypredict,test$Conference)
misclassification <- 1 - (sum(diag(confusion_mat))/sum(confusion_mat))
ypredict <- predict(object = elastic_model,newx = predictor_test, s = cv.fit$lambda.min,
type = "class", exact = TRUE)
ypredict
ypredict <- predict(object = elastic_model,newx = predictor_test, s = cv.fit$lambda.min,
type = "class", exact = TRUE)
predictor_test <- as.matrix(test[,-4703])
ypredict <- predict(object = elastic_model,newx = predictor_test, s = cv.fit$lambda.min,
type = "class", exact = TRUE)
confusion_mat <- table(ypredict,test$Conference)
library(Matrix)
coef(ypredict)
ypredict
coef(ypredict)
ypredict
=cv.fit$lambda.min
as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
a<- as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
nonzeroCoef(a)
nonzero(a)
nonzeroCoef(a)
length(a)
nonzeroCoef(a, bystep= FALSE)
elastic_model[-1,]
coefficients(cv.fit)[-1,]
a<- as.numeric(coef(cv.fit, s=cv.fit$lambda.min))
nonzeroCoef(a, bystep= FALSE)
cat(paste0(names(a[a != 0]),"\n"))
a<-coefficients(cv.fit)[-1,]
cat(paste0(names(a[a != 0]),"\n"))
a<-coef(cv.fit)[-1,]
cat(paste0(names(a[a != 0]),"\n"))
a<- as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
cat(paste0(names(a[a != 0]),"\n"))
cat(paste0(a[a != 0],"\n"))
length(a[a!=0])
a<- coef(elastic_model, s=cv.fit$lambda.min)
cat(paste0(a[a != 0],"\n"))
a<- coef(elastic_model, s=cv.fit$lambda.min)
cat(paste0(a[a != 0],"\n"))
cat(paste0(names(a[a != 0],"\n")))
cat(paste0(name(a[a != 0],"\n")))
cat(paste0(names(a[a != 0]),"\n"))
a<- as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
length(a[a != 0])
a<- as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
selected_coeff<- as.numeric(coef(elastic_model, s=cv.fit$lambda.min))
number_of_feature <- length(selected_coeff[selected_coeff != 0])
misclassification_elastic
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
rownames(train) <- 1:nrow(train)
which(colnames(train)=="Conference")
x <- t(train[,-4703])
y <- train[[4703]]
test_x <- t(test[,-4703])
mydata <- list(x=x, y= as.factor(y), geneid=as.character(1:nrow(x)),genenames = rownames(x))
model_train <- pamr.train(mydata)
cv_model <- pamr.cv(model_train, data = mydata)
cv_model <- pamr.cv(model_train, data = mydata)
print(cv_model)
model_fit <- pamr.train(mydata, threshold = cv_model$threshold[which.min(cv_model$error)])
features = pamr.listgenes(model_train, mydata, threshold = 1.306, genenames=TRUE)
no_of_feature_cen <- nrow(features)
no_of_feature_cen <- nrow(features)
no_of_feature_cen
library(kernlab)
data1
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
data_email$Conference <- as.factor(data$Conference)
n=dim(data_email)[1]
set.seed(12345)
# 70% Training Data
id=sample(1:n, floor(n*0.7))
train=data_email[id,]
# 30% validation & testing Data
test = data_email[-id,]
pvals <- c()
data1
data1[,1]
data1[,2]
data1[,3]
pvals <- c()
for (i in 1:length(data1)) {
ttest <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
debugSource('~/Documents/projects/machine-learning-labs/Lab-2-block-2/assignment 2.R', echo=TRUE)
t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")$pvalue
t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")$pval
t <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
t$p.value
t <- t.test(data1[i] ~ Conference, data = data1, alternative = "two.sided")
t <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
for (i in 1:length(data1)) {
ttest <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
pvals <- c()
for (i in 1:length(data1)) {
ttest <- t.test(data1[i], data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
pvalues_df <- data.frame(p_value=pvals,feature=1:length(data1))
pvalues_df <- pvalues_df[order(pvalues_df$p_value),]
ALPHA <- 0.05
L <- c()
it <- 1
# Let alpha = 0.05  ## ask oleg
for (j in 1:nrow(pvalues_df)) {
if(pvalues_df$p_value[j] < ALPHA * (j / nrow(pvalues_df)) )
{
L[it] <- j
it <- it +1
}
}
max(L)
LL = pvalues[max(L)]
LL
newPvalues <- c()
pvalue_feature <- c()
pvalue_status <- c()
j<- 1
for (j in 1:nrow(pvalues_df))
{
pvalue_status[j] <- TRUE
newPvalues[j] <- pvalues_df$p_value[j]
pvalue_feature[j] <- pvalues_df$feature[j]
if(pvalues[j] <= LL)
{
pvalue_status[j] <- FALSE
}
}
result <- data.frame(p_value=newPvalues, feature=pvalue_feature,status=pvalue_status)
rejected_features <- c()
k <- 1
for (j in 1:ncol(data1)) {
if(result$status[j] == FALSE)
{
rejected_features[k] <- colnames(data1[result$feature[j]])
k<- k + 1
}
}
rejected_features
library(ggplot2)
p <- ggplot(result, aes(x=index, y=p_value)) + geom_point(aes(color=status), size=1 ) +
scale_color_manual (values =  c('blue', 'red')) +
labs(x="CL carspace length", y="RW rear Width", colour="Classes")
# +
# geom_abline(slope = glm_slope, intercept = glm_intercept) + ggtitle("Logistic Regression decision Boundary")
p
BH = p.adjust(pvalues, "BH")
BH
plot(BH)
pvals <- c()
for (i in 1:length(data1)) {
ttest <- t.test(data1[i], data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
pvalues_df <- data.frame(p_value=pvals,feature=1:length(data1))
pvalues_df <- pvalues_df[order(pvalues_df$p_value),]
ALPHA <- 0.05
L <- c()
it <- 1
# Let alpha = 0.05  ## ask oleg
for (j in 1:nrow(pvalues_df)) {
if(pvalues_df$p_value[j] < ALPHA * (j / nrow(pvalues_df)) )
{
L[it] <- j
it <- it +1
}
}
max(L)
LL = pvalues_df$p_value[max(L)]
LL
newPvalues <- c()
pvalue_feature <- c()
pvalue_status <- c()
j<- 1
for (j in 1:nrow(pvalues_df))
{
pvalue_status[j] <- TRUE
newPvalues[j] <- pvalues_df$p_value[j]
pvalue_feature[j] <- pvalues_df$feature[j]
if(pvalues_df$p_value[j] <= LL)
{
pvalue_status[j] <- FALSE
}
}
result <- data.frame(p_value=newPvalues, feature=pvalue_feature,status=pvalue_status)
result
rejected_features <- c()
k <- 1
for (j in 1:ncol(data1)) {
if(result$status[j] == FALSE)
{
rejected_features[k] <- colnames(data1[result$feature[j]])
k<- k + 1
}
}
rejected_features
## The features that corresponds to the rejected hypothesis are the
# ones which are most relevant to "announcement of conference", that
# is which contributes the most for the classification of announcement of conference
data1[-4073]
for (i in 1:length(data1)-1) {
ttest <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
pvalues_df <- data.frame(p_value=pvals,feature=1:length(data1)-1)
pvalues_df <- pvalues_df[order(pvalues_df$p_value),]
ALPHA <- 0.05
L <- c()
it <- 1
# Let alpha = 0.05  ## ask oleg
for (j in 1:nrow(pvalues_df)) {
if(pvalues_df$p_value[j] < ALPHA * (j / nrow(pvalues_df)) )
{
L[it] <- j
it <- it +1
}
}
max(L)
LL = pvalues_df$p_value[max(L)]
LL
newPvalues <- c()
pvalue_feature <- c()
pvalue_status <- c()
j<- 1
for (j in 1:nrow(pvalues_df))
{
pvalue_status[j] <- TRUE
newPvalues[j] <- pvalues_df$p_value[j]
pvalue_feature[j] <- pvalues_df$feature[j]
if(pvalues_df$p_value[j] <= LL)
{
pvalue_status[j] <- FALSE
}
}
result <- data.frame(p_value=newPvalues, feature=pvalue_feature,status=pvalue_status)
result
rejected_features <- c()
k <- 1
for (j in 1:ncol(data1)-1) {
if(result$status[j] == FALSE)
{
rejected_features[k] <- colnames(data1[result$feature[j]])
k<- k + 1
}
}
rejected_features
## The features that corresponds to the rejected hypothesis are the
# ones which are most relevant to "announcement of conference", that
# is which contributes the most for the classification of announcement of conference
rejected_features
rejected_features
pvals <- c()
for (i in 1:(length(data1)-1)) {
ttest <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
length(data1)
class(data1)
ncol(data1)
data <- read.csv2("data.csv", header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, check.names = FALSE)
data1 <- as.data.frame(data)
data_email <- as.data.frame(data)
pvals <- c()
for (i in 1:(ncol(data1)-1)) {
ttest <- t.test(data1[,i] ~ Conference, data = data1, alternative = "two.sided")
pvals[i] <- ttest$p.value
}
pvalues_df <- data.frame(p_value=pvals,feature=(1:length(data1)-1))
length(pvals)
pvalues_df <- data.frame(p_value=pvals,feature=(1:ncol(data1)-1))
(1:ncol(data1)-1)
(1:(ncol(data1)-1)
)
ncol(data1)
pvalues_df <- data.frame(p_value=pvals,feature=(1:(ncol(data1)-1) )
pvalues_df <- pvalues_df[order(pvalues_df$p_value),]
pvalues_df <- data.frame(p_value=pvals,feature=(1:(ncol(data1)-1) )
;
pvalues_df <- data.frame(p_value=pvals,feature=1:(ncol(data1)-1) )
pvalues_df <- pvalues_df[order(pvalues_df$p_value),]
ALPHA <- 0.05
L <- c()
it <- 1
# Let alpha = 0.05  ## ask oleg
for (j in 1:nrow(pvalues_df)) {
if(pvalues_df$p_value[j] < ALPHA * (j / nrow(pvalues_df)) )
{
L[it] <- j
it <- it +1
}
}
max(L)
LL = pvalues_df$p_value[max(L)]
LL
newPvalues <- c()
pvalue_feature <- c()
pvalue_status <- c()
j<- 1
for (j in 1:nrow(pvalues_df))
{
pvalue_status[j] <- TRUE
newPvalues[j] <- pvalues_df$p_value[j]
pvalue_feature[j] <- pvalues_df$feature[j]
if(pvalues_df$p_value[j] <= LL)
{
pvalue_status[j] <- FALSE
}
}
result <- data.frame(p_value=newPvalues, feature=pvalue_feature,status=pvalue_status)
result
rejected_features <- c()
k <- 1
for (j in 1:(ncol(data1)-1))  {
if(result$status[j] == FALSE)
{
rejected_features[k] <- colnames(data1[result$feature[j]])
k<- k + 1
}
}
rejected_features
cat(paste(rejected_features, collapse = '\n'))
View(pvalues_df)
LL
L
pvalues_df
ggplot(data = pvalues_df, aes(x = 1:4702,y=pvalues_df$p_value, col = reject)) + geom_point() + labs(x="feature",y="p-value")
rejected_features <- c()
k <- 1
for (j in 1:(ncol(data1)-1))  {
if(result$status[j] == FALSE)
{
rejected_features[k] <- colnames(data1[result$feature[j]])
pvalues_df$status <- rejected_features[k]
k<- k + 1
}
}
View(pvalues_df)
result
ggplot(data = result, aes(x = 1:4702,y=result$p_value, col = status)) + geom_point() + labs(x="feature",y="p-value")
cat(paste(rejected_features, collapse = '\n'))
library(kernlab)
x <- as.matrix(train[,-4703])
y <- train[,4703]
svm_fit <- ksvm(data = train,Conference ~ . ,kernel="vanilladot",
scaled = FALSE)
svm_fit
